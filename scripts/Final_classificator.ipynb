{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np                               \n",
    "import pandas as pd                        \n",
    "import matplotlib.pyplot as plt                  \n",
    "import cv2             \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "from PIL import Image\n",
    "from skimage import exposure\n",
    "import os                                        \n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical          \n",
    "from tensorflow.keras.models import load_model                                 \n",
    "import warnings\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = '/gpfs/space/home/markusha/project/output/dataframes/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataframe from pickled pandas object\n",
    "dataframe1 = pd.read_pickle(output_path + 'Kaggle_data_fixed.pkl')\n",
    "dataframe2 = pd.read_pickle(output_path + 'Larssen_data1.pkl')\n",
    "dataframe3 = pd.read_pickle(output_path + 'Larssen_data2.pkl')\n",
    "dataframe4 = pd.read_pickle(output_path + 'GTSRB_data.pkl')\n",
    "dataframe5 = pd.read_pickle(output_path + 'Belgium_data_fixed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = dataframe1[['sign','img_path']].append(dataframe2[['sign','img_path']]).append(dataframe3[['sign','img_path']]).append(dataframe4[['sign','img_path']]).append(dataframe5[['sign','img_path']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_data['sign'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "labels = np.array([])\n",
    "data_clahe = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, image_file in training_data.iterrows():\n",
    "    image = cv2.imread(image_file['img_path'])\n",
    "    #resizing\n",
    "    image = cv2.resize(image, (50,50), interpolation = cv2.INTER_AREA)\n",
    "    image = np.array(image)\n",
    "    # using clahe algorithm\n",
    "    image_p = exposure.equalize_adapthist(image, clip_limit=0.1)\n",
    "    data_clahe.append(image_p)\n",
    "    # data = np.append(data,image)\n",
    "    data.append(image)\n",
    "    labels = np.append(labels, image_file['sign'])\n",
    "data_clahe = np.array(data_clahe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(map(lambda x: x.replace(' ', ''), labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((np.unique(labels)))\n",
    "classes = len(np.unique(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to One-Hot Encode the labels\n",
    "char_to_int = dict((c, i) for i, c in enumerate(np.unique(labels)))\n",
    "labels_integer_encoded = [char_to_int[char] for char in labels]\n",
    "print(char_to_int)\n",
    "labels_1hot = to_categorical(labels_integer_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# shuffeling data\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "data_clahe, labels_1hot = shuffle(np.array(data_clahe), np.array(labels_1hot))\n",
    "\n",
    "# predicting with CLAHE data\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_clahe, labels_1hot, test_size=0.2, random_state=69)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "classes = y_train.shape[1]\n",
    "print(\"# classes: \" + str(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "inputShape = X_train.shape[1:]\n",
    "activation_method1 = \"relu\"\n",
    "activation_method2 = \"softmax\"\n",
    "padding_method = \"same\"\n",
    "model2.add(Conv2D(8, (5, 5), padding=padding_method,input_shape=inputShape))\n",
    "model2.add(Activation(activation_method1))\n",
    "model2.add(BatchNormalization(axis=-1))\n",
    "model2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model2.add(Conv2D(16, (3, 3), padding=padding_method))\n",
    "model2.add(Activation(activation_method1))\n",
    "model2.add(BatchNormalization(axis=-1))\n",
    "model2.add(Conv2D(16, (3, 3), padding=padding_method))\n",
    "model2.add(Activation(activation_method1))\n",
    "model2.add(BatchNormalization(axis=-1))\n",
    "model2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model2.add(Conv2D(32, (3, 3), padding=padding_method))\n",
    "model2.add(Activation(activation_method))\n",
    "model2.add(BatchNormalization(axis=-1))\n",
    "model2.add(Conv2D(32, (3, 3), padding=padding_method))\n",
    "model2.add(Activation(activation_method1))\n",
    "model2.add(BatchNormalization(axis=-1))\n",
    "model2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model2.add(Flatten())\n",
    "model2.add(Dense(128))\n",
    "model2.add(Activation(activation_method1))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(Dropout(0.5))\n",
    "model2.add(Flatten())\n",
    "model2.add(Dense(128))\n",
    "model2.add(Activation(activation_method1))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(Dropout(0.5))\n",
    "model2.add(Dense(classes))\n",
    "model2.add(Activation(activation_method2))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model2.fit(X_train, y_train, batch_size=64, epochs=40, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model2.save(\"Trafic_signs_model_final2.h5\")\n",
    "#plotting graphs for accuracy \n",
    "plt.figure(0)\n",
    "plt.plot(history.history['accuracy'], label='training accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='val accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "#plotting graphs for loss \n",
    "plt.figure(1)\n",
    "plt.plot(history.history['loss'], label='training loss')\n",
    "plt.plot(history.history['val_loss'], label='val loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model2 = load_model('Trafic_signs_model_final.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model2.predict(data_clahe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_1hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_char = {k:v for k,v in zip(char_to_int.values(),char_to_int.keys())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_integer = [np.argmax(x) for x in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels_predicted = [int_to_char[integer] for integer in list(predictions_integer)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_integer_encoded = [np.argmax(x) for x in labels_1hot]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_true = [int_to_char[integer] for integer in labels_integer_encoded]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prediction_to_label(model, X_test, char_to_int):\n",
    "#     predictions = model.predict(X_test)\n",
    "#     predictions = [np.argmax(x) for x in predictions]\n",
    "#     int_to_char = {k:v for k,v in zip(char_to_int.values(),char_to_int.keys())}\n",
    "#     labels_predicted = [int_to_char[integer] for integer in predictions]\n",
    "#     return labels_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a_file = open(\"char_to_int.pkl\", \"wb\")\n",
    "# pickle.dump(char_to_int, a_file)\n",
    "# a_file.close()\n",
    "\n",
    "## open as \n",
    "#a_file = open(\"char_to_int.pkl\", \"rb\")\n",
    "#output = pickle.load(a_file)\n",
    "#print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description\n",
    "\n",
    "# # Let's predict the whole dataset\n",
    "# data = np.array(data_clahe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = model2.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions2 = [np.argmax(x) for x in predictions] #index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " prediction_strength = [predictions[i][j] for i,j in enumerate(predictions_integer)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'label' : labels_true, 'predicted':labels_predicted, 'strength':prediction_strength})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['correct'] = list(df['label'] ==df['predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df[df.predicted=='PRIORITY_ROAD'][df.correct == False].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.predicted=='PRIORITY_ROAD'].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['correct']][['label', 'strength']].groupby('label',as_index=False).mean().sort_values('strength',ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['correct']][['label', 'strength']].groupby('label',as_index=False).mean().sort_values('strength',ascending = True).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['correct'] == False][['label', 'strength']].groupby('label',as_index=False).mean().sort_values('strength',ascending = True).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['correct'] == False][['label', 'strength']].groupby('label',as_index=False).mean().sort_values('strength',ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(char_to_int.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tester = np.argmax(labels_1hot, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_tester, y_pred, target_names =list(char_to_int.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "index = 101\n",
    "print(\"Label: \" + labels_true[index])\n",
    "print(\"Predicted: \" + labels_predicted[index])\n",
    "plt.imshow(data_clahe[index])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_example",
   "language": "python",
   "name": "venv_example"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
